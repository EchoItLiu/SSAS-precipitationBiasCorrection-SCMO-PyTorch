{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.alexnet.AlexNet'>\n",
      "AlexNet(\n",
      "  61.101 M, 100.000% Params, 0.716 GMac, 100.000% MACs, \n",
      "  (features): Sequential(\n",
      "    2.47 M, 4.042% Params, 0.657 GMac, 91.804% MACs, \n",
      "    (0): Conv2d(0.023 M, 0.038% Params, 0.07 GMac, 9.848% MACs, 3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs, inplace=True)\n",
      "    (2): MaxPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(0.307 M, 0.503% Params, 0.224 GMac, 31.316% MACs, 64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.020% MACs, inplace=True)\n",
      "    (5): MaxPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.020% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(0.664 M, 1.087% Params, 0.112 GMac, 15.681% MACs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.009% MACs, inplace=True)\n",
      "    (8): Conv2d(0.885 M, 1.448% Params, 0.15 GMac, 20.902% MACs, 384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs, inplace=True)\n",
      "    (10): Conv2d(0.59 M, 0.966% Params, 0.1 GMac, 13.936% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs, inplace=True)\n",
      "    (12): MaxPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    58.631 M, 95.958% Params, 0.059 GMac, 8.195% MACs, \n",
      "    (0): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.5, inplace=False)\n",
      "    (1): Linear(37.753 M, 61.788% Params, 0.038 GMac, 5.276% MACs, in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, inplace=True)\n",
      "    (3): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.5, inplace=False)\n",
      "    (4): Linear(16.781 M, 27.465% Params, 0.017 GMac, 2.345% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, inplace=True)\n",
      "    (6): Linear(4.097 M, 6.705% Params, 0.004 GMac, 0.573% MACs, in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "ResNet50 |flops: 0.72 GMac |params: 61.1 M\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import alexnet\n",
    "model = resnet50()\n",
    "model_alex = alexnet()\n",
    "\n",
    "print (type(model_alex))\n",
    "\n",
    "# 实例化model对象后直接调用下面代码\n",
    "flops, params = get_model_complexity_info(model_alex, (3,224,224),as_strings=True,print_per_layer_stat=True)\n",
    "\n",
    "print(\"%s |flops: %s |params: %s\" % ('ResNet50',flops,params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from collections import Counter\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 relu = True, bn = False, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, bias=False, **kwargs)\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu:\n",
    "            # \n",
    "            x = F.leaky_relu(x, inplace = True)\n",
    "        return x\n",
    "                                                   \n",
    "                                                   \n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class Means(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Means, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.mean(input, dim=(1, 2, 3)).unsqueeze(-1)\n",
    "\n",
    "        \n",
    "class ZeroOuts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ZeroOuts, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchSize = x.size()[0]\n",
    "        return torch.zeros(batchSize, 4, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__._FCN'>\n",
      "FCN: |flops: 0.16 GMac |params: 638.66 k\n"
     ]
    }
   ],
   "source": [
    "class _FCN(nn.Module):\n",
    "    def __init__(self, in_channel, hidden_channel): \n",
    "        super(_FCN, self).__init__()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.encoder = nn.Sequential(\n",
    "            BasicConv2d(in_channel, hidden_channel, 1, bn = True),\n",
    "            BasicConv2d(hidden_channel, hidden_channel, 3, bn = True, padding = 1),\n",
    "            BasicConv2d(hidden_channel, 128, 1, bn = True), \n",
    "        )\n",
    "        self.shortcut = BasicConv2d(in_channel, 128, 1, bn=True)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(128 * 1 * 1, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "        )            \n",
    "             \n",
    "            \n",
    "    def forward(self, x):                                     \n",
    "        res = self.shortcut(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x + res\n",
    "        fcn_x = self.pool(x)\n",
    "        fcn_x = self.fc(fcn_x)\n",
    "        return fcn_x\n",
    "    \n",
    "###\n",
    "fcn = _FCN(\n",
    "    27,\n",
    "    256,\n",
    ")\n",
    "\n",
    "print (type(fcn))\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops, params = get_model_complexity_info(fcn, (27,16,16),as_strings=True,print_per_layer_stat=False)\n",
    "    \n",
    "print(\"%s |flops: %s |params: %s\" % ('FCN:', flops, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__._MLP'>\n",
      "MLP: |flops: 0.0 GMac |params: 182.72 k\n"
     ]
    }
   ],
   "source": [
    "class _MLP(nn.Module):\n",
    "    def __init__(self, in_channel, hidden_channel): \n",
    "        super(_MLP, self).__init__()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.mlp = nn.Sequential(\n",
    "             nn.Linear(in_channel, 512),\n",
    "             nn.Linear(512, 256),\n",
    "             nn.Linear(256, 128), \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 1 * 1, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "        )            \n",
    "             \n",
    "            \n",
    "    def forward(self, x):                                     \n",
    "        x = self.mlp(x)\n",
    "        y = self.fc(x)\n",
    "        return y\n",
    "    \n",
    "###\n",
    "mlp = _MLP(\n",
    "    27,\n",
    "    256,\n",
    ")\n",
    "\n",
    "print (type(mlp))\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops, params = get_model_complexity_info(mlp, (1,27), as_strings=True, print_per_layer_stat=False)\n",
    "print(\"%s |flops: %s |params: %s\" %('MLP:',flops, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__._FPN'>\n",
      "FPN: |flops: 0.11 GMac |params: 3.16 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yqliu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2796: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/yqliu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "class bottleneck_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, curr_stride):\n",
    "        super(bottleneck_block, self).__init__()        \n",
    "        self.shortcut = BasicConv2d(in_channel, out_channel, 1, bn=True, stride=curr_stride)\n",
    "        # Bottleneck\n",
    "        self.bk = nn.Sequential(\n",
    "            BasicConv2d(in_channel, 64, 1, bn=True),\n",
    "            # change scale\n",
    "            BasicConv2d(64, 64, 3, bn=True, stride=curr_stride, padding=1),\n",
    "            BasicConv2d(64, out_channel, 1, bn=True),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        x = self.bk(x)\n",
    "        b_x = x + res \n",
    "        return b_x\n",
    "\n",
    "class _FPN(nn.Module):\n",
    "    def __init__(self, in_channel, hidden_channel):\n",
    "        super(_FPN, self).__init__()\n",
    "        # 57 - 64\n",
    "        self.smooth_s = BasicConv2d(in_channel, hidden_channel, 3, bn = True, padding=1)\n",
    "        # 64 - 64 stride[1,1]\n",
    "        self.c1 = nn.Sequential(\n",
    "            bottleneck_block(hidden_channel, hidden_channel*2, 1),\n",
    "            bottleneck_block(hidden_channel*2, hidden_channel, 1),\n",
    "        )\n",
    "        ## 瓶颈网络: 一大一小\n",
    "        # 64 - 128 stride[2,1]\n",
    "        self.c2 = nn.Sequential(\n",
    "            bottleneck_block(hidden_channel, hidden_channel*4, 2),\n",
    "            bottleneck_block(hidden_channel*4, hidden_channel*2, 1),\n",
    "        )\n",
    "        # 128 - 256 stride[2,1]\n",
    "        self.c3 = nn.Sequential(\n",
    "            bottleneck_block(hidden_channel*2, hidden_channel*8, 2),\n",
    "            bottleneck_block(hidden_channel*8, hidden_channel*4, 1),\n",
    "        )        \n",
    "        # 256 - 512 stride[2,1]\n",
    "        self.c4 = nn.Sequential(\n",
    "            bottleneck_block(hidden_channel*4, hidden_channel*16, 2),\n",
    "            bottleneck_block(hidden_channel*16, hidden_channel*8, 1),\n",
    "        ) \n",
    "        #\n",
    "        self.top = BasicConv2d(hidden_channel*8, hidden_channel*4, 3, bn = True, padding=1)\n",
    "        self.latlayer1 = BasicConv2d(hidden_channel*4, hidden_channel*4, 1, bn = True)\n",
    "        self.latlayer2 = BasicConv2d(hidden_channel*2, hidden_channel*4, 1, bn = True)\n",
    "        self.latlayer3 = BasicConv2d(hidden_channel, hidden_channel*4, 1, bn = True)\n",
    "        self.smooth_e = BasicConv2d(hidden_channel*4, hidden_channel, 3, bn = True, padding=1)\n",
    "        #\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # fc\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(hidden_channel * 1 * 1, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        \n",
    "    def res(self, p, c):\n",
    "        _,_,H,W = c.size()\n",
    "        res_cp = c + F.upsample(p, size=(H,W), mode='bilinear')\n",
    "        return res_cp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Bottom-up\n",
    "        c1 = self.smooth_s(x) \n",
    "        c2 = self.c1(c1)\n",
    "        c3 = self.c2(c2)\n",
    "        c4 = self.c3(c3)\n",
    "        c5 = self.c4(c4)\n",
    "        # Top-down\n",
    "        p5 = self.top(c5)\n",
    "        # fuse and hidden-smooth \n",
    "        p4 = self.res(p5, self.latlayer1(c4))\n",
    "        p3 = self.res(p4, self.latlayer2(c3))\n",
    "        p2 = self.res(p3, self.latlayer3(c2))\n",
    "        # Smooth\n",
    "        p2 = self.smooth_e(p2)\n",
    "        # pooling HW for p2\n",
    "        p2 = self.pool(p2)\n",
    "        # last layer in p\n",
    "        pred = self.fc(p2)                   \n",
    "        return pred\n",
    "    \n",
    "\n",
    "###\n",
    "fpn = _FPN(\n",
    "    27,\n",
    "    64,\n",
    ")\n",
    "\n",
    "print (type(fpn))\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops, params = get_model_complexity_info(fpn, (27,16,16), as_strings=True, print_per_layer_stat=False)\n",
    "    \n",
    "print(\"%s |flops: %s |params: %s\" % ('FPN:', flops, params))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM: |flops: 13.08 GMac |params: 9.43 M\n"
     ]
    }
   ],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, tsLength, inputChannelNums, hiddenChannelNums, ecsize, device, layer_flag):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        if layer_flag==True:\n",
    "            self.ts_conv = nn.Conv2d(in_channels=(inputChannelNums + hiddenChannelNums),\n",
    "                                     out_channels=hiddenChannelNums * 4,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "            \n",
    "        else:\n",
    "            self.ts_conv = nn.Conv2d(in_channels=hiddenChannelNums * 2,\n",
    "                                     out_channels=hiddenChannelNums * 4,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)            \n",
    "        \n",
    "           \n",
    "        self.ts_bn = nn.BatchNorm2d(hiddenChannelNums * 4)\n",
    "        # smooth\n",
    "        self._conv = nn.Sequential(\n",
    "             nn.Conv2d(in_channels=hiddenChannelNums, out_channels=hiddenChannelNums, kernel_size=3, stride=1, padding = 1)\n",
    "#             BasicConv2d(128, 128, 3,  padding = 1),\n",
    "#             BasicConv2d(128, 32, 3, padding =1),\n",
    "#             BasicConv2d(32, 32, 1),\n",
    "        )\n",
    "        \n",
    "        self.ac = nn.ReLU(True)        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(hiddenChannelNums, 1),\n",
    "        )\n",
    "        #\n",
    "        self._state_height, self._state_width = ecsize, ecsize\n",
    "        # LSTM W\n",
    "        self.W_ci = nn.Parameter(torch.zeros(1, hiddenChannelNums, self._state_height, self._state_width))\n",
    "        self.W_cf = nn.Parameter(torch.zeros(1, hiddenChannelNums, self._state_height, self._state_width))\n",
    "        self.W_co = nn.Parameter(torch.zeros(1, hiddenChannelNums, self._state_height, self._state_width))\n",
    "        # LSTM B\n",
    "        self.b_i = nn.Parameter(torch.zeros(1, hiddenChannelNums, self._state_height, self._state_width))\n",
    "        self.b_f = nn.Parameter(torch.zeros(1, hiddenChannelNums, self._state_height, self._state_width))\n",
    "        self.b_c = nn.Parameter(torch.zeros(1, hiddenChannelNums, self._state_height, self._state_width))\n",
    "        self.b_o = nn.Parameter(torch.zeros(1, hiddenChannelNums, self._state_height, self._state_width))\n",
    "        #\n",
    "        self._seq = tsLength \n",
    "        self._input_channel =  inputChannelNums\n",
    "        self._hidden_channel_nums = hiddenChannelNums\n",
    "        #\n",
    "#         self.device = device\n",
    "        self.layer_flag = layer_flag\n",
    "               \n",
    "    # inputs: N * C * D * h * w  \n",
    "    # num_filter is channel number of h \n",
    "    def forward(self, inputs = None, states = None):\n",
    "        # D * N * C * h * w origin inputs        \n",
    "        if self.layer_flag==True:\n",
    "            inputs = inputs.transpose(0,2).transpose(1,2)\n",
    "        # \n",
    "        else:   \n",
    "            inputs = torch.stack(inputs)            \n",
    "\n",
    "        if states is None:\n",
    "            # LSTM Cell and hidden\n",
    "            c = torch.zeros((inputs.size(1), self._hidden_channel_nums, self._state_height,\n",
    "                                  self._state_width), dtype = torch.float)\n",
    "            h = torch.zeros((inputs.size(1), self._hidden_channel_nums, self._state_height,\n",
    "                             self._state_width), dtype = torch.float)\n",
    "            \n",
    "        else:\n",
    "            h, c = states\n",
    "\n",
    "        outputs = []\n",
    "        #\n",
    "        for index in range(self._seq):\n",
    "            if inputs is None:\n",
    "                x = torch.zeros((h.size(0), self._input_channel, self._state_height,\n",
    "                                      self._state_width), dtype=torch.float)\n",
    "            else:\n",
    "                x = inputs[index]\n",
    "                \n",
    "            st_x = torch.cat([x, h], dim = 1)\n",
    "            #\n",
    "            conved_st_x = self.ts_conv(st_x)\n",
    "            conved_st_x_bn = self.ts_bn(conved_st_x)\n",
    "            conv_i, conv_f, conv_c, conv_o = torch.chunk(conved_st_x_bn, 4, dim = 1)\n",
    "            # save spatiotemporal features\n",
    "            i = torch.sigmoid(conv_i + self.W_ci * c + self.b_i)\n",
    "            f = torch.sigmoid(conv_f + self.W_cf * c + self.b_f)\n",
    "            # c_{t-1} → c_t\n",
    "            c = f * c + i * torch.tanh(conv_c + self.b_c)\n",
    "            #\n",
    "            o = torch.sigmoid(conv_o + self.W_co * c + self.b_o)\n",
    "            h = o * torch.tanh(c)\n",
    "            \n",
    "            # output y_hat\n",
    "            x_deep = self._conv(h)\n",
    "            x_relu = self.ac(x_deep)\n",
    "            x_pooling = self.pooling(x_relu)\n",
    "            y_hat_TimeStamp_last = self.fc(x_pooling)\n",
    "            #\n",
    "            outputs.append(h)        \n",
    "        # N * hiC * h * w (hidden in the last timeStamp) | N * 1                    \n",
    "        return outputs, y_hat_TimeStamp_last\n",
    "    \n",
    "    \n",
    "class _ConvLSTM(nn.Module):\n",
    "    def __init__(self, tsLength, inputChannelNums, hiddenChannelNums, ec_size, devices):\n",
    "        super(_ConvLSTM, self).__init__()\n",
    "        self._w_h = ec_size\n",
    "#         self.device = devices                \n",
    "        # ts representation - stacked 2 layers\n",
    "        self.encoderConvLSTM_layer1 = ConvLSTMCell(tsLength, inputChannelNums, hiddenChannelNums, ec_size, devices, True)\n",
    "        #\n",
    "        self.encoderConvLSTM_layer2 = ConvLSTMCell(tsLength, inputChannelNums, hiddenChannelNums, ec_size, devices, False)        \n",
    "    def forward(self, x): \n",
    "        # ConvLSTM\n",
    "        outputsL1, _ = self.encoderConvLSTM_layer1(x, states = None)\n",
    "        _, ts_y_last = self.encoderConvLSTM_layer2(outputsL1, states = None)                             \n",
    "        return ts_y_last\n",
    "    \n",
    "\n",
    "###\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "###    \n",
    "conv_lstm = _ConvLSTM(\n",
    "    6,\n",
    "    27,\n",
    "    256,\n",
    "    16,\n",
    "    device     \n",
    "    )\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops, params = get_model_complexity_info(conv_lstm, (27,6,16,16), as_strings=True, print_per_layer_stat=False)\n",
    "    \n",
    "print(\"%s |flops: %s |params: %s\" % ('ConvLSTM:',flops, params)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoModel: |flops: 0.03 GMac |params: 96.43 k\n",
      "rainFallClassifierModel: |flops: 0.25 GMac |params: 303.39 k\n"
     ]
    }
   ],
   "source": [
    "\"\"\"------- single base classifer for ordinal-------\"\"\"\n",
    "class BaseClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseClassifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicConv2d(64, 128, 3, padding = 1),\n",
    "            BasicConv2d(128, 128, 3, padding = 1),\n",
    "            BasicConv2d(128, 32, 1),\n",
    "            BasicConv2d(32, 32, 1),\n",
    "        )\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "        self.downsampler = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 1, padding=0),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.ac = nn.ReLU(True)\n",
    "        # sigmoid for multi-labels and outputing a probability\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "# 3\n",
    "class OrdinalRegressionModel(nn.Module):\n",
    "    def __init__(self, nClass):\n",
    "        super(OrdinalRegressionModel, self).__init__()\n",
    "        self.nClass = nClass\n",
    "        self.boosting = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.nClass):\n",
    "            oneClassifier = BaseClassifier()\n",
    "            self.boosting.append(oneClassifier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # list sigmoid outputs from all classifers\n",
    "        outputs = [self.boosting[i](x) for i in range(self.nClass)]\n",
    "        # list → Tensor (torch.Size([1, nClass])\n",
    "        return torch.cat(outputs, dim = 1)\n",
    "    \n",
    "# 2\n",
    "class rainFallClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rainFallClassification, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicConv2d(57, 64, 3, bn = True, padding=1),\n",
    "            BasicConv2d(64, 64, 3, bn = True, padding=1),\n",
    "            BasicConv2d(64, 128, 3, bn = True, padding=1),\n",
    "            BasicConv2d(128, 128, 3, bn = True, padding=1),\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            BasicConv2d(57, 128, 1, bn = True, relu = False, padding=0)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(128 * 1 * 1, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = x + residual\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 1\n",
    "class AutoencoderBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoencoderBN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            BasicConv2d(57, 32, 1, bn = True),\n",
    "            BasicConv2d(32, 32, 3, bn = True, padding = 1),\n",
    "        )\n",
    "        \n",
    "        self.encoderAfterNoise = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            # -------------------------------------\n",
    "            BasicConv2d(32, 64, 3, bn = True, padding = 1),\n",
    "            BasicConv2d(64, 64, 3, bn = True, padding = 1),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            BasicConv2d(64, 32, 3, bn = True, padding = 1),\n",
    "            # upsample and output size → C × 29 × 29 \n",
    "            nn.Upsample(size = (29, 29), mode ='bilinear',align_corners = True),\n",
    "            BasicConv2d(32, 32, 3, bn = True, padding = 1),\n",
    "            \n",
    "            BasicConv2d(32, 57, 1, bn = True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder = self.encoder(x)\n",
    "        encoder = self.encoderAfterNoise(encoder)\n",
    "        decoder = self.decoder(encoder)\n",
    "        return encoder, decoder\n",
    "    \n",
    "autoModel = AutoencoderBN()\n",
    "regressionModel = OrdinalRegressionModel(71)\n",
    "rainFallClassifierModel = rainFallClassification()   \n",
    "# bs = BaseClassifier()\n",
    "\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops_1, params_1 = get_model_complexity_info(autoModel, (57,29,29), as_strings=True, print_per_layer_stat=False)\n",
    "# 71\n",
    "# flops_2, params_2 = get_model_complexity_info(bs, (64,29,29), as_strings=True, print_per_layer_stat=False)\n",
    "flops_3, params_3 = get_model_complexity_info(rainFallClassifierModel, (57,29,29), as_strings=True, print_per_layer_stat=False)\n",
    "     \n",
    "    \n",
    "# MLP: |flops: 0.0 GMac |params: 177.09 k\n",
    "\n",
    "print(\"%s |flops: %s |params: %s\" % ('autoModel:',flops_1, params_1)) \n",
    "# print(\"%s |flops: %s |params: %s\" % ('BaseClassifier:',flops_2, params_2)) \n",
    "print(\"%s |flops: %s |params: %s\" % ('rainFallClassifierModel:', flops_3, params_3))\n",
    "\n",
    "# ECB 0.0071+0.03+0.25 = 0.29 GMac | params 96.43 + 303.39 + 177.09 * 71 = 12.67M\n",
    "# ERA5 0.0071 + 0.02 + 0.07 = 0.097 GMac | params 95.47 + 282.27 + 177.09 * 71 = 12.65M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3AE_3DI(\n",
      "  0.572 M, 100.000% Params, 0.359 GMac, 100.000% MACs, \n",
      "  (Conv_3d): BasicConv3d(\n",
      "    0.074 M, 12.922% Params, 0.359 GMac, 99.862% MACs, \n",
      "    (conv): Conv3d(0.074 M, 12.899% Params, 0.358 GMac, 99.688% MACs, 64, 64, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn): BatchNorm3d(0.0 M, 0.022% Params, 0.001 GMac, 0.173% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc_prob): Linear(0.498 M, 87.074% Params, 0.0 GMac, 0.138% MACs, in_features=20736, out_features=24, bias=True)\n",
      "  (fc_pred): Linear(0.0 M, 0.004% Params, 0.0 GMac, 0.000% MACs, in_features=24, out_features=1, bias=True)\n",
      ")\n",
      "C3AE_3DII(\n",
      "  2.106 M, 100.000% Params, 0.361 GMac, 100.000% MACs, \n",
      "  (Conv_3d): BasicConv3d(\n",
      "    0.074 M, 3.507% Params, 0.359 GMac, 99.437% MACs, \n",
      "    (conv): Conv3d(0.074 M, 3.501% Params, 0.358 GMac, 99.265% MACs, 64, 64, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn): BatchNorm3d(0.0 M, 0.006% Params, 0.001 GMac, 0.172% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc_wd_prob): Linear(0.394 M, 18.707% Params, 0.0 GMac, 0.109% MACs, in_features=20736, out_features=19, bias=True)\n",
      "  (fc_ws_prob): Linear(1.638 M, 77.782% Params, 0.002 GMac, 0.454% MACs, in_features=20736, out_features=79, bias=True)\n",
      "  (fc_wd_pred): Linear(0.0 M, 0.001% Params, 0.0 GMac, 0.000% MACs, in_features=19, out_features=1, bias=True)\n",
      "  (fc_ws_pred): Linear(0.0 M, 0.004% Params, 0.0 GMac, 0.000% MACs, in_features=79, out_features=1, bias=True)\n",
      ")\n",
      "tem_MSM: |flops: 0.21 GMac |params: 348.16 k\n",
      "wind_MSM: |flops: 0.21 GMac |params: 344.26 k\n",
      "tem_MTM: |flops: 0.36 GMac |params: 571.57 k\n",
      "wind_MTM: |flops: 0.36 GMac |params: 2.11 M\n"
     ]
    }
   ],
   "source": [
    "from pre_training.deformConv.layers import ConvOffset2D\n",
    "\n",
    "\n",
    "## util classes\n",
    "class BasicConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 relu = True, bn = False, **kwargs):\n",
    "        super(BasicConv3d, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=False, **kwargs)\n",
    "        \n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm3d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu:\n",
    "            # \n",
    "            x = F.leaky_relu(x, inplace = True)\n",
    "        return x\n",
    "\n",
    "## Pre-training\n",
    "\n",
    "# C3AE_3DI   \n",
    "class C3AE_3DI(nn.Module):\n",
    "    def __init__(self, seq, Range, curscale, ED_out_channel):\n",
    "        super(C3AE_3DI, self).__init__()\n",
    "        self.curscale = curscale\n",
    "        prob_len = len(Range)\n",
    "        # kernal depth 2 and shared weight\n",
    "        self.Conv_3d = BasicConv3d(ED_out_channel, 64, (2,3,3), bn=True, padding=(0,1,1))\n",
    "        # Pooling\n",
    "#         self.pooling  = nn.AdaptiveAvgPool2d(output_size=(1, 1))     \n",
    "        # crop\n",
    "        self.fc_prob = nn.Linear(64*curscale*curscale, prob_len)\n",
    "        self.fc_pred = nn.Linear(prob_len, 1)\n",
    "                                  \n",
    "    def forward(self, x):\n",
    "        # N1*C1*D*H_u*W_u: D → D-1 ... → 1\n",
    "        flag = 0\n",
    "        while (x.size(2)) >1:\n",
    "            x = self.Conv_3d(x)\n",
    "#             print ('curscale:', x.size(2))\n",
    "        x = torch.flatten(x, start_dim=1)        \n",
    "        # N1 * len(range)                         \n",
    "        prob = self.fc_prob(x)\n",
    "        # N1                         \n",
    "        pred = self.fc_pred(prob)                          \n",
    "        return prob, pred\n",
    "\n",
    "class DeformConvNetI(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(DeformConvNetI, self).__init__()\n",
    "        \n",
    "        # shallow feature map\n",
    "        self.shortcut = BasicConv2d(in_channel, 256, 1, bn=True, padding=0)\n",
    "        # Bottleneck\n",
    "        self.bk = nn.Sequential(\n",
    "            BasicConv2d(256, 64, 1, bn=True),\n",
    "            BasicConv2d(64, 64, 3, bn=True, padding=1),\n",
    "            BasicConv2d(64, 256, 1, bn=True),\n",
    "            )\n",
    "        # smooth\n",
    "        self.smo = BasicConv2d(256, 32, 3, bn=True, padding=1)\n",
    "        \n",
    "        # Dconv1\n",
    "        self.offset1 = ConvOffset2D(32)\n",
    "        self.conv1 = BasicConv2d(32, 64, 3, bn=True, padding=1)\n",
    "\n",
    "#         # Dconv2\n",
    "#         self.offset2 = ConvOffset2D(64)\n",
    "#         self.conv2 = BasicConv2d(64, 128, 3, bn=True, padding=1)\n",
    "\n",
    "        # Dconv3\n",
    "        self.offset3 = ConvOffset2D(64)\n",
    "        self.conv3 = BasicConv2d(64, 128, 3, bn=True, padding=1)\n",
    "\n",
    "        # Pooling\n",
    "        self.pooling  = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        \n",
    "        # MLP\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Dropout(0.5),            \n",
    "            nn.Linear(128,32),\n",
    "            nn.Linear(32, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        x = self.bk(res) + res\n",
    "        \n",
    "        x = self.smo(x)\n",
    "  \n",
    "        x = self.offset1(x)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "#         x = self.offset2(x)\n",
    "#         x = self.conv2(x)\n",
    "        \n",
    "        x = self.offset3(x)\n",
    "        x = self.conv3(x)        \n",
    "        \n",
    "        x = self.pooling(x)  \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# class DeformConvNetII        \n",
    "class DeformConvNetII(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(DeformConvNetII, self).__init__()\n",
    "        \n",
    "        # shallow feature map\n",
    "        self.shortcut = BasicConv2d(in_channel, 256, 1, bn=True, padding=0)\n",
    "        # Bottleneck\n",
    "        self.bk = nn.Sequential(\n",
    "            BasicConv2d(256, 64, 1, bn=True),\n",
    "            BasicConv2d(64, 64, 3, bn=True, padding=1),\n",
    "            BasicConv2d(64, 256, 1, bn=True),\n",
    "            )\n",
    "        # smooth\n",
    "        self.smo = BasicConv2d(256, 32, 3, bn=True, padding=1)\n",
    "        \n",
    "        # Dconv1\n",
    "        self.offset1 = ConvOffset2D(32)\n",
    "        self.conv1 = BasicConv2d(32, 64, 3, bn=True, padding=1)\n",
    "\n",
    "        # Dconv2\n",
    "#         self.offset2 = ConvOffset2D(64)\n",
    "#         self.conv2 = BasicConv2d(64, 128, 3, bn=True, padding=1)\n",
    "\n",
    "        # Dconv3\n",
    "        self.offset3 = ConvOffset2D(64)\n",
    "        self.conv3 = BasicConv2d(64, 128, 3, bn=True, padding=1)\n",
    "        # Pooling\n",
    "        self.pooling  = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        \n",
    "        # MLP branch (2 tasks)\n",
    "        self.flat = Flatten()\n",
    "        self.fc_dre = nn.Linear(128, 1)\n",
    "        self.fc_voc = nn.Linear(128, 1)        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        x = self.bk(res) + res\n",
    "        \n",
    "        x = self.smo(x)\n",
    "  \n",
    "        x = self.offset1(x)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "#         x = self.offset2(x)\n",
    "#         x = self.conv2(x)\n",
    "        \n",
    "        x = self.offset3(x)\n",
    "        x = self.conv3(x)        \n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.flat(x)\n",
    "        x_dre = self.fc_dre(x)\n",
    "        x_voc = self.fc_voc(x)    \n",
    "        x_total = torch.cat((x_dre, x_dre),-1) \n",
    "        return x_total  \n",
    "    \n",
    "\n",
    "class C3AE_3DII(nn.Module):\n",
    "    def __init__(self, seq, wdRange, wsRange, curscale, ED_out_channel):\n",
    "        super(C3AE_3DII, self).__init__()\n",
    "        self.curscale = curscale\n",
    "        prob_len_wd = len(wdRange)\n",
    "        prob_len_ws = len(wsRange)\n",
    "        \n",
    "        self.curscale = curscale\n",
    "        \n",
    "        # kernal depth 2 and shared weight\n",
    "        self.Conv_3d = BasicConv3d(ED_out_channel, 64, (2,3,3), bn=True, padding=(0,1,1))\n",
    "        \n",
    "        # Pooling\n",
    "#         self.pooling  = nn.AdaptiveAvgPool2d(output_size=(1, 1))     \n",
    "        # crop\n",
    "        self.fc_wd_prob = nn.Linear(64*curscale*curscale, prob_len_wd)\n",
    "        self.fc_ws_prob = nn.Linear(64*curscale*curscale, prob_len_ws)                   \n",
    "        self.fc_wd_pred = nn.Linear(prob_len_wd, 1)\n",
    "        self.fc_ws_pred = nn.Linear(prob_len_ws, 1)\n",
    "                                                                    \n",
    "    def forward(self, x):\n",
    "        # N1*C1*D*H_u*W_u: D → D-1 ... → 1\n",
    "        flag = 0\n",
    "        while (x.size(2)) >1:\n",
    "            x = self.Conv_3d(x)\n",
    "#             print ('curscale:', x.size(2))\n",
    "        x = torch.flatten(x, start_dim=1)        \n",
    "        # N1 * len(range) \n",
    "        # N1*C(HW_u)                         \n",
    "        prob_wd = self.fc_wd_prob(x)\n",
    "        prob_ws = self.fc_ws_prob(x)\n",
    "        # N1                         \n",
    "        pred_wd = self.fc_wd_pred(prob_wd)\n",
    "        pred_ws = self.fc_ws_pred(prob_ws)              \n",
    "        return prob_wd, pred_wd, prob_ws, pred_ws\n",
    "    \n",
    "tem_MSM = DeformConvNetI(57)\n",
    "wind_MSM = DeformConvNetII(57)\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops_1, params_1 = get_model_complexity_info(tem_MSM, (57,29,29), as_strings=True, print_per_layer_stat=False)\n",
    "# 71\n",
    "flops_2, params_2 = get_model_complexity_info(wind_MSM, (57,29,29), as_strings=True, print_per_layer_stat=False)\n",
    "\n",
    "\n",
    "tem_MTM = C3AE_3DI(6, np.arange(-4,43,2), 18,  64)\n",
    "wind_MTM = C3AE_3DII(6, np.arange(1,20,1), np.arange(5,400,5), 18, 64)\n",
    "\n",
    "flops_3, params_3 = get_model_complexity_info(tem_MTM, (64,6,18,18), as_strings=True, print_per_layer_stat=True)\n",
    "# 71\n",
    "flops_4, params_4 = get_model_complexity_info(wind_MTM, (64,6,18,18), as_strings=True, print_per_layer_stat=True)\n",
    "\n",
    "print(\"%s |flops: %s |params: %s\" % ('tem_MSM:',flops_1, params_1)) \n",
    "print(\"%s |flops: %s |params: %s\" % ('wind_MSM:',flops_2, params_2)) \n",
    "print(\"%s |flops: %s |params: %s\" % ('tem_MTM:',flops_3, params_3)) \n",
    "print(\"%s |flops: %s |params: %s\" % ('wind_MTM:',flops_4, params_4)) \n",
    "\n",
    "\n",
    "# ECB 0.21*5 = 1.05 GMac | params: 348.16 = 0.34 * 5  = 0.17 M\n",
    "# ERA5 0.36 * 5 = 1.80 GMac | params 571.57 = 0.56 * 4 +2.11 = 4.35 M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cb8bae4dcfaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0mrainFallClassifierModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrainFallClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m \u001b[0mflops_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_complexity_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_per_layer_stat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;31m# flops_2, params_2 = get_model_complexity_info(ED_DCNN, (256,6,29,29), as_strings=True, print_per_layer_stat=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;31m# flops_3, params_3 = get_model_complexity_info(CNN_3D, (64,6,18,18), as_strings=True, print_per_layer_stat=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ptflops/flops_counter.py\u001b[0m in \u001b[0;36mget_model_complexity_info\u001b[0;34m(model, input_res, print_per_layer_stat, as_strings, input_constructor, ost, verbose, ignore_modules, custom_modules_hooks)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_res\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_res\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mCUSTOM_MODULES_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mCUSTOM_MODULES_MAPPING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_modules_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from collections import Counter\n",
    "from pre_training.deformConv.layers import ConvOffset2D\n",
    "\n",
    "\n",
    "## Training\n",
    "class BasicConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 relu = True, bn = False, **kwargs):\n",
    "        super(BasicConv3d, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=False, **kwargs)\n",
    "        \n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm3d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu:\n",
    "            # \n",
    "            x = F.leaky_relu(x, inplace = True)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 relu = True, bn = False, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, bias=False, **kwargs)\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu:\n",
    "            # \n",
    "            x = F.leaky_relu(x, inplace = True)\n",
    "        return x\n",
    "                                                   \n",
    "                                                   \n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class Means(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Means, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.mean(input, dim=(1, 2, 3)).unsqueeze(-1)\n",
    "\n",
    "        \n",
    "class ZeroOuts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ZeroOuts, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchSize = x.size()[0]\n",
    "        return torch.zeros(batchSize, 4, 1, 1)\n",
    "\n",
    "    \n",
    "\"\"\"------- single base classifer for ordinal-------\"\"\"\n",
    "class BaseClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseClassifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicConv2d(64, 128, 3, padding = 1),\n",
    "            BasicConv2d(128, 128, 3, padding = 1),\n",
    "            BasicConv2d(128, 32, 1),\n",
    "            BasicConv2d(32, 32, 1),\n",
    "        )\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "        self.downsampler = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 1, padding=0),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.ac = nn.ReLU(True)\n",
    "        # sigmoid for multi-labels and outputing a probability\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "\n",
    "## 1\n",
    "class uniEncoder(nn.Module):\n",
    "    def __init__(self, uniScale, in_channel):\n",
    "        super(uniEncoder, self).__init__()\n",
    "        self.uniScale = uniScale\n",
    "        # 考虑one sample BN是否得False\n",
    "        self.shortcut = BasicConv2d(in_channel, 256, 1, bn=True, padding=0)\n",
    "        # Bottleneck\n",
    "        self.bk = nn.Sequential(\n",
    "            BasicConv2d(256, 64, 1, bn=True),\n",
    "            BasicConv2d(64, 64, 3, bn=True, padding=1),\n",
    "            BasicConv2d(64, 256, 1, bn=True),\n",
    "            )\n",
    "        # smooth\n",
    "        self.smo = BasicConv2d(256, 256, 3, bn=True, padding=1) \n",
    "        # uniscale-downsampling - find key factor in big region\n",
    "        self.uniDown = nn.AdaptiveMaxPool2d(uniScale)   \n",
    "        \n",
    "    def forward(self, x, Bs, flag, isTrain):\n",
    "        if flag=='D-1':\n",
    "            cpu_uni_stats_l = []\n",
    "            for stat_cuda in x:\n",
    "                stat_cuda = self.shortcut(stat_cuda)\n",
    "                feature_stat = self.bk(stat_cuda) + stat_cuda\n",
    "                feature_stat = self.smo(feature_stat)\n",
    "                if feature_stat.size(2)>=self.uniScale:\n",
    "                    uni_stat = self.uniDown(feature_stat) \n",
    "                if feature_stat.size(2)<self.uniScale:\n",
    "                    uni_stat = F.interpolate(feature_stat, size=self.uniScale, mode='bilinear')\n",
    "                if isTrain:\n",
    "                    uni_stat = uni_stat.detach()\n",
    "                    \n",
    "                cpu_uni_stats_l.append(uni_stat.cpu().numpy())\n",
    "            # N(D-1)*1*C*H*W → N(D-1)*C*H*W → N*(D-1)*C*H*W → N*C*(D-1)*H*W  \n",
    "            cpu_uni_stats = np.stack(cpu_uni_stats_l).squeeze(1)\n",
    "            channel_size = cpu_uni_stats.shape[1]\n",
    "            #\n",
    "            cpu_uni_stats = np.reshape(cpu_uni_stats, (Bs, -1, channel_size, self.uniScale, self.uniScale))\n",
    "            cpu_uni_stats = np.transpose(cpu_uni_stats, (0,2,1,3,4))\n",
    "           \n",
    "            return cpu_uni_stats\n",
    "        ##     \n",
    "        if flag=='D-th':\n",
    "            # N*C*H*W\n",
    "            x = self.shortcut(x)\n",
    "            feature_stat = self.bk(x) + x\n",
    "            feature_stat = self.smo(feature_stat)\n",
    "            uni_stat = self.uniDown(feature_stat)\n",
    "            return uni_stat\n",
    "        \n",
    "        \n",
    "## 2     \n",
    "class ED_DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ED_DCNN, self).__init__()\n",
    "        # Dconv1 has noisy\n",
    "        self.offset1 = ConvOffset2D(256)\n",
    "        self.conv1 = BasicConv2d(256, 64, 3, bn=True, padding=1)\n",
    "        # Dconv2\n",
    "        self.offset2 = ConvOffset2D(64)\n",
    "        self.conv2 = BasicConv2d(64, 64, 3, bn=True, padding=1)\n",
    "        \n",
    "    def forward(self, x, Bs, isTrain):\n",
    "        cpu_deform_stats_l = []\n",
    "        # N * C * D * H * W → N * D * C * H * W → ND * C * H * W\n",
    "        feature_cha = x.size(1)\n",
    "        hw = x.size(3)\n",
    "        ND_x = x.transpose(1,2).contiguous().view(-1, feature_cha, hw, hw)\n",
    "        for statUni_cuda in ND_x:\n",
    "            # batchsize = 1\n",
    "            statUni_cuda = statUni_cuda.unsqueeze(0)\n",
    "            statdf_cuda = self.offset1(statUni_cuda)\n",
    "            statdf_cuda = self.conv1(statdf_cuda)\n",
    "            statdf_cuda = self.offset2(statdf_cuda)\n",
    "            statdf_cuda = self.conv2(statdf_cuda)\n",
    "            if isTrain:\n",
    "                statdf_cuda = statdf_cuda.detach()            \n",
    "            #\n",
    "            cpu_deform_stats_l.append(statdf_cuda.cpu().numpy())\n",
    "        # ND*1*C*H*W → ND*C*H*W → N*D*C*H*W → N*C*D*H*W  \n",
    "        cpu_deform_stats = np.stack(cpu_deform_stats_l).squeeze(1)\n",
    "        channel_size = cpu_deform_stats.shape[1]\n",
    "        cpu_deform_stats = np.reshape(cpu_deform_stats, (Bs, -1, channel_size, hw, hw))\n",
    "        cpu_deform_stats = np.transpose(cpu_deform_stats, (0,2,1,3,4))        \n",
    "        return cpu_deform_stats \n",
    "\n",
    "\n",
    "## 3    \n",
    "class CNN_3D(nn.Module):\n",
    "    def __init__(self, curscale, out_channel_3d):\n",
    "        super(CNN_3D, self).__init__()\n",
    "        self.curscale = curscale\n",
    "        self.Conv_3d = BasicConv3d(64, out_channel_3d, (2,3,3), bn=True, padding=(0,1,1))\n",
    "                                  \n",
    "    def forward(self, x, isTrain):\n",
    "        cpu_out_stats_l = []\n",
    "        for stat_ts in x:\n",
    "#             stat_ts = stat_ts.unsqueeze(0)\n",
    "            # 1*C1*D*H_u*W_u: D → D-1 ... → 1            \n",
    "            while (stat_ts.size(2)) >1:\n",
    "                stat_ts = self.Conv_3d(stat_ts)                \n",
    "            if isTrain:\n",
    "                stat_ts = stat_ts.detach()\n",
    "            \n",
    "            cpu_out_stats_l.append(stat_ts.cpu().numpy())\n",
    "        # N*1*C*H*W → N*C*H*W\n",
    "        cpu_out_stats = np.stack(cpu_out_stats_l).squeeze(1).squeeze(2)\n",
    "        out_stats =  torch.from_numpy(cpu_out_stats.astype(np.float32)).cuda()\n",
    "        return out_stats \n",
    "\n",
    "    \n",
    "## 4\n",
    "class OrdinalRegressionModel(nn.Module):\n",
    "    def __init__(self, nClass):\n",
    "        super(OrdinalRegressionModel, self).__init__()\n",
    "        self.nClass = nClass\n",
    "        self.boosting = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.nClass):\n",
    "            oneClassifier = BaseClassifier()\n",
    "            self.boosting.append(oneClassifier)\n",
    "\n",
    "    def forward(self, x): \n",
    "#         print ('self.nClass:', self.nClass)\n",
    "        # list sigmoid outputs from all classifers\n",
    "        outputs = [self.boosting[i](x) for i in range(self.nClass)]\n",
    "        # list → Tensor (torch.Size([1, nClass])\n",
    "        return torch.cat(outputs, dim = 1)\n",
    "        \n",
    "## 5\n",
    "class rainFallClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rainFallClassification, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicConv2d(57, 64, 3, bn = True, padding=1),\n",
    "            BasicConv2d(64, 64, 3, bn = True, padding=1),\n",
    "            BasicConv2d(64, 128, 3, bn = True, padding=1),\n",
    "            BasicConv2d(128, 128, 3, bn = True, padding=1),\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            BasicConv2d(57, 128, 1, bn = True, relu = False, padding=0)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(128 * 1 * 1, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = x + residual\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)   \n",
    "uniE = uniEncoder(18, 57)\n",
    "EDCNN = ED_DCNN()\n",
    "CNN3D = CNN_3D(18, 64)\n",
    "regressionModel = OrdinalRegressionModel(71)    \n",
    "rainFallClassifierModel = rainFallClassification()\n",
    "\n",
    "flops_1, params_1 = get_model_complexity_info(uniEncoder, (57,29,29), as_strings=True, print_per_layer_stat=False)\n",
    "# flops_2, params_2 = get_model_complexity_info(ED_DCNN, (256,6,29,29), as_strings=True, print_per_layer_stat=False)\n",
    "# flops_3, params_3 = get_model_complexity_info(CNN_3D, (64,6,18,18), as_strings=True, print_per_layer_stat=False)\n",
    "# flops_4, params_4 = get_model_complexity_info(OrdinalRegressionModel, (64,6,18,18), as_strings=True, print_per_layer_stat=False)\n",
    "# flops_5, params_5 = get_model_complexity_info(rainFallClassifierModel, (64,6,18,18), as_strings=True, print_per_layer_stat=False)\n",
    "\n",
    "\n",
    "print(\"%s |flops: %s |params: %s\" % ('uniE:',flops_1, params_1)) \n",
    "# print(\"%s |flops: %s |params: %s\" % ('EDCNN:',flops_2, params_2)) \n",
    "# print(\"%s |flops: %s |params: %s\" % ('CNN3D:',flops_3, params_3)) \n",
    "# print(\"%s |flops: %s |params: %s\" % ('regressionModel:',flops_4, params_4)) \n",
    "# print(\"%s |flops: %s |params: %s\" % ('rainFallClassifierModel:',flops_5, params_5)) \n",
    "\n",
    "\n",
    "\n",
    "# ECb1 0.21*5 = 1.05 GMac | params: 348.16 = 0.34 * 5  = 0.17 M\n",
    "# ERA5 0.36 * 5 = 1.80 GMac | params 571.57 = 0.56 * 4 +2.11 = 4.35 M\n",
    "\n",
    "# ECb2 9.43 + 9.43/2  + \n",
    "# ERA5 2\n",
    "\n",
    "# ConvLSTM: |flops: 13.08 GMac |params: 9.43 M\n",
    "\n",
    "# 0.17 + 9.43 + 9.43/2 = 14.32\n",
    "\n",
    "# 0.17 + 9.43 + 9.43/2 = xx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM: |flops: 0.0 GMac |params: 497.79 k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0.50G  1.82M\\n\\n0.21G    \\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 记得方法加nn.DataParallel\n",
    "class _LSTM(nn.Module):\n",
    "    def __init__(self,in_channel, hidden_channel):\n",
    "        super(_LSTM, self).__init__()\n",
    "        self.in_channel = in_channel        \n",
    "        self.hidden_channel = hidden_channel\n",
    "        self.hidden_channel_after = hidden_channel-128\n",
    "        #\n",
    "        self.lstm1 = nn.LSTMCell(in_channel, hidden_channel)        \n",
    "        self.lstm2 = nn.LSTMCell(hidden_channel, self.hidden_channel_after)        \n",
    "        #\n",
    "#         self.dropout = nn.Dropout(p=0.3)\n",
    "        self.linear1 = nn.Linear(self.hidden_channel_after, 64)\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # D*N*f\n",
    "        seq_length = x.size(0)\n",
    "        #\n",
    "        h_t_1 = torch.zeros(x.size(1), self.hidden_channel)\n",
    "        c_t_1 = torch.zeros(x.size(1), self.hidden_channel)\n",
    "        h_t_2 = torch.zeros(x.size(1), self.hidden_channel_after)\n",
    "        c_t_2 = torch.zeros(x.size(1), self.hidden_channel_after)          \n",
    "        for i in range(seq_length):\n",
    "            h_t_1, c_t_1 = self.lstm1(x[i], (h_t_1, c_t_1))\n",
    "            h_t_2, c_t_2 = self.lstm2(h_t_1, (h_t_2, c_t_2))\n",
    "            \n",
    "        h21 = self.linear1(h_t_2)\n",
    "        #\n",
    "        y_t = self.linear2(h21).squeeze()\n",
    "\n",
    "        return y_t\n",
    "    \n",
    "\n",
    "###\n",
    "\n",
    "###    \n",
    "lstm = _LSTM(\n",
    "        27,\n",
    "        256)\n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops, params = get_model_complexity_info(lstm, (6,27), as_strings=True, print_per_layer_stat=False)\n",
    "    \n",
    "print(\"%s |flops: %s |params: %s\" % ('LSTM:',flops, params))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "0.50G  1.82M\n",
    "\n",
    "0.21G  1.71M  \n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "'''\n",
    "Z = AXW\n",
    "'''\n",
    "    def __init__(self , A, dim_in , dim_out):\n",
    "        super(GCN,self).__init__()\n",
    "        self.A = A\n",
    "        self.fc1 = nn.Linear(dim_in ,dim_in,bias=False)\n",
    "        self.fc2 = nn.Linear(dim_in,dim_in//2,bias=False)\n",
    "        self.fc3 = nn.Linear(dim_in//2,dim_out,bias=False)\n",
    "\n",
    "    def forward(self,X):\n",
    "        '''\n",
    "        计算三层gcn\n",
    "        '''\n",
    "        X = F.relu(self.fc1(self.A.mm(X)))\n",
    "        X = F.relu(self.fc2(self.A.mm(X)))\n",
    "        return self.fc3(self.A.mm(X))\n",
    "    \n",
    "gcn = GCN(\n",
    "        57,\n",
    "        256)   \n",
    "\n",
    "# HR-ECb 57 × 6 × 29 × 29 \n",
    "# (57,29,29)\n",
    "# ERA5 27 × 6 × 16 × 16 \n",
    "# (27,16,16)\n",
    "flops, params = get_model_complexity_info(gcn, (6,27), as_strings=True, print_per_layer_stat=False)\n",
    "    \n",
    "print(\"%s |flops: %s |params: %s\" % ('GCN:',flops, params))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pError_ele = [1,0,3,0,5]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
